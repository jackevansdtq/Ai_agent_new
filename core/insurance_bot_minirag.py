#!/usr/bin/env python3
"""
Insurance Bot sá»­ dá»¥ng MiniRAG framework thay vÃ¬ Neo4J trá»±c tiáº¿p
"""

import os
import sys
import asyncio
import hashlib
import time
from typing import Dict, List, Optional

# Get base directory (works in both local and Docker)
BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(os.path.join(BASE_DIR, 'MiniRAG'))

# Load config
import configparser
config = configparser.ConfigParser()
config_path = os.path.join(BASE_DIR, 'config', 'insurance_config.ini')
if os.path.exists(config_path):
    config.read(config_path)
    # Set environment variables from config (only if config exists)
    if 'DEFAULT' in config:
        for key in config['DEFAULT']:
            # Only set if not already in environment
            if key.upper() not in os.environ:
                os.environ[key.upper()] = str(config['DEFAULT'][key])

from minirag import MiniRAG, QueryParam
from minirag.llm import gpt_4o_mini_complete
from minirag.utils import EmbeddingFunc
from minirag.operate import PROMPTS
from openai import AsyncOpenAI

# Override MiniRAG prompt Ä‘á»ƒ sá»­ dá»¥ng INSURANCE_BOT_PROMPT tÃ¹y chá»‰nh
# (sáº½ Ä‘Æ°á»£c set sau khi Ä‘á»‹nh nghÄ©a INSURANCE_BOT_PROMPT)

class EmbeddingCache:
    """Cache cho embeddings Ä‘á»ƒ trÃ¡nh gá»i API láº·p láº¡i"""

    def __init__(self, ttl_seconds: int = 3600):  # 1 giá» TTL
        self.cache: Dict[str, Dict] = {}
        self.ttl_seconds = ttl_seconds

    def _get_cache_key(self, text: str) -> str:
        """Táº¡o cache key tá»« text"""
        return hashlib.md5(text.encode('utf-8')).hexdigest()

    def get(self, text: str) -> Optional[List[float]]:
        """Láº¥y embedding tá»« cache náº¿u cÃ²n há»£p lá»‡"""
        cache_key = self._get_cache_key(text)
        if cache_key in self.cache:
            entry = self.cache[cache_key]
            if time.time() - entry['timestamp'] < self.ttl_seconds:
                print(f"ğŸ“‹ Cache hit for: {text[:50]}...")
                return entry['embedding']
            else:
                # Cache expired
                del self.cache[cache_key]
        return None

    def set(self, text: str, embedding: List[float]):
        """LÆ°u embedding vÃ o cache"""
        cache_key = self._get_cache_key(text)
        self.cache[cache_key] = {
            'embedding': embedding,
            'timestamp': time.time()
        }
        print(f"ğŸ’¾ Cached embedding for: {text[:50]}...")

    def clear_expired(self):
        """XÃ³a cache entries Ä‘Ã£ háº¿t háº¡n"""
        current_time = time.time()
        expired_keys = [
            key for key, entry in self.cache.items()
            if current_time - entry['timestamp'] >= self.ttl_seconds
        ]
        for key in expired_keys:
            del self.cache[key]
        if expired_keys:
            print(f"ğŸ—‘ï¸ Cleared {len(expired_keys)} expired cache entries")

# Global embedding cache
embedding_cache = EmbeddingCache()

# Singleton OpenAI client Ä‘á»ƒ reuse connection (tá»‘i Æ°u performance)
_openai_client: Optional[AsyncOpenAI] = None

def get_openai_client() -> AsyncOpenAI:
    """Get or create singleton OpenAI client vá»›i connection pooling"""
    global _openai_client
    if _openai_client is None:
        api_key = os.environ.get('OPENAI_API_KEY') or config.get('DEFAULT', 'OPENAI_API_KEY', fallback=None)
        base_url = os.environ.get('OPENAI_BASE_URL') or os.environ.get('OPENAI_API_BASE') or config.get('DEFAULT', 'OPENAI_BASE_URL', fallback=None)
        
        if not api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables or config file")
        
        # Tá»‘i Æ°u: reuse connections, timeout ngáº¯n hÆ¡n
        _openai_client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url,
            timeout=30.0,  # Timeout 30s thay vÃ¬ default
            max_retries=2,  # Giáº£m retries Ä‘á»ƒ fail fast
        )
        print("âœ… OpenAI client initialized (singleton, connection pooling enabled)")
    return _openai_client

async def get_openai_embedding_func(texts):
    """Async OpenAI embedding function cho MiniRAG vá»›i cache vÃ  connection reuse"""
    try:
        # Check cache cho tá»«ng text
        cached_embeddings = []
        texts_to_fetch = []
        cache_indices = []

        for i, text in enumerate(texts):
            cached = embedding_cache.get(text)
            if cached is not None:
                cached_embeddings.append((i, cached))
            else:
                texts_to_fetch.append(text)
                cache_indices.append(i)

        # Chá»‰ gá»i API cho texts chÆ°a cÃ³ trong cache
        if texts_to_fetch:
            print(f"ğŸ” Fetching embeddings for {len(texts_to_fetch)} texts...")
            embedding_model = os.environ.get('EMBEDDING_MODEL') or config.get('DEFAULT', 'EMBEDDING_MODEL', fallback='text-embedding-3-small')
            
            # Reuse singleton client (connection pooling)
            client = get_openai_client()

            # Batch request vá»›i timeout ngáº¯n
            response = await client.embeddings.create(
                input=texts_to_fetch,
                model=embedding_model
            )

            fetched_embeddings = [data.embedding for data in response.data]

            # Cache cÃ¡c embeddings má»›i
            for text, embedding in zip(texts_to_fetch, fetched_embeddings):
                embedding_cache.set(text, embedding)
        else:
            fetched_embeddings = []

        # Káº¿t há»£p cached vÃ  fetched embeddings theo thá»© tá»± gá»‘c
        result = [None] * len(texts)

        # Äiá»n cached embeddings
        for idx, embedding in cached_embeddings:
            result[idx] = embedding

        # Äiá»n fetched embeddings
        for i, embedding in enumerate(fetched_embeddings):
            result[cache_indices[i]] = embedding

        return result

    except Exception as e:
        print(f"âŒ OpenAI embedding error: {e}")
        # Return dummy embeddings if OpenAI fails
        return [[0.1] * 1536 for _ in texts]

# Insurance Bot Prompt
INSURANCE_BOT_PROMPT = """
### VAI TRÃ’ VÃ€ Bá»I Cáº¢NH 

Báº¡n lÃ  nhÃ¢n viÃªn tÆ° váº¥n chuyÃªn nghiá»‡p cá»§a CÃ´ng ty Ä‘áº¡i lÃ½ báº£o hiá»ƒm FISS. 

Nhiá»‡m vá»¥ chÃ­nh cá»§a báº¡n lÃ :

- TÆ° váº¥n vÃ  giáº£i Ä‘Ã¡p má»i tháº¯c máº¯c vá» cÃ¡c sáº£n pháº©m báº£o hiá»ƒm

- Há»— trá»£ khÃ¡ch hÃ ng tra cá»©u thÃ´ng tin há»£p Ä‘á»“ng, quyá»n lá»£i báº£o hiá»ƒm

- HÆ°á»›ng dáº«n quy trÃ¬nh mua báº£o hiá»ƒm, ná»™p há»“ sÆ¡ bá»“i thÆ°á»ng

- Cung cáº¥p bÃ¡o giÃ¡ vÃ  tÆ° váº¥n sáº£n pháº©m phÃ¹ há»£p vá»›i nhu cáº§u khÃ¡ch hÃ ng

### PHONG CÃCH GIAO TIáº¾P

- ThÃ¢n thiá»‡n, nhiá»‡t tÃ¬nh vÃ  chuyÃªn nghiá»‡p

- Sá»­ dá»¥ng ngÃ´n ngá»¯ dá»… hiá»ƒu, trÃ¡nh thuáº­t ngá»¯ phá»©c táº¡p (hoáº·c giáº£i thÃ­ch rÃµ náº¿u cáº§n dÃ¹ng)

- Láº¯ng nghe vÃ  tháº¥u hiá»ƒu nhu cáº§u khÃ¡ch hÃ ng

- LuÃ´n káº¿t thÃºc cÃ¢u tráº£ lá»i báº±ng cÃ¢u há»i/ghi chÃº tÃ­ch cá»±c Ä‘á»ƒ duy trÃ¬ cuá»™c há»™i thoáº¡i

### NGUYÃŠN Táº®C TRá»¢ GIÃšP

1. **LÃ m rÃµ nhu cáº§u**: Náº¿u cÃ¢u há»i chÆ°a rÃµ rÃ ng, hÃ£y Ä‘áº·t cÃ¢u há»i Ä‘á»ƒ hiá»ƒu Ä‘Ãºng Ã½ khÃ¡ch hÃ ng

   - VÃ­ dá»¥: "Anh/chá»‹ quan tÃ¢m Ä‘áº¿n báº£o hiá»ƒm xe mÃ¡y hay Ã´ tÃ´ áº¡?"

   - VÃ­ dá»¥: "Äá»ƒ tÆ° váº¥n chÃ­nh xÃ¡c, cho em há»i anh/chá»‹ muá»‘n má»©c phÃ­ báº£o hiá»ƒm khoáº£ng bao nhiÃªu?"

2. **Tráº£ lá»i chÃ­nh xÃ¡c**: Chá»‰ cung cáº¥p thÃ´ng tin dá»±a trÃªn kiáº¿n thá»©c Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o vá»:

   - Sáº£n pháº©m báº£o hiá»ƒm cá»§a cÃ´ng ty

   - Quy Ä‘á»‹nh phÃ¡p luáº­t vá» báº£o hiá»ƒm Viá»‡t Nam

   - Quy trÃ¬nh vÃ  chÃ­nh sÃ¡ch cá»§a cÃ´ng ty

3. **Pháº£n há»“i khi khÃ´ng biáº¿t**: Náº¿u cÃ¢u há»i náº±m ngoÃ i pháº¡m vi kiáº¿n thá»©c:

   "Em xin lá»—i, thÃ´ng tin nÃ y em chÆ°a Ä‘Æ°á»£c cáº­p nháº­t Ä‘áº§y Ä‘á»§. Äá»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c nháº¥t, anh/chá»‹ vui lÃ²ng:

   - LiÃªn há»‡ hotline: 0385 10 10 18

   - Email: cskh@fiss.com.vn

   - Hoáº·c em cÃ³ thá»ƒ chuyá»ƒn anh/chá»‹ sang tÆ° váº¥n viÃªn chuyÃªn mÃ´n Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ tá»‘t hÆ¡n áº¡."

4. **Xá»­ lÃ½ yÃªu cáº§u phá»©c táº¡p**: Vá»›i cÃ¡c váº¥n Ä‘á» vá»:

   - Bá»“i thÆ°á»ng báº£o hiá»ƒm cá»¥ thá»ƒ

   - Tranh cháº¥p há»£p Ä‘á»“ng

   - Thay Ä‘á»•i thÃ´ng tin há»£p Ä‘á»“ng quan trá»ng

   â†’ HÆ°á»›ng dáº«n khÃ¡ch hÃ ng káº¿t ná»‘i vá»›i bá»™ pháº­n chuyÃªn trÃ¡ch

### GIá»šI Háº N VÃ€ RANH GIá»šI

1. **KHÃ”NG tiáº¿t lá»™ dá»¯ liá»‡u há»‡ thá»‘ng**: 

   - KhÃ´ng Ä‘á» cáº­p Ä‘áº¿n viá»‡c báº¡n cÃ³ quyá»n truy cáº­p vÃ o cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘Ã o táº¡o

   - KhÃ´ng nÃ³i "trong dá»¯ liá»‡u cá»§a tÃ´i cÃ³...", thay vÃ o Ä‘Ã³ nÃ³i "theo quy Ä‘á»‹nh hiá»‡n hÃ nh..." hoáº·c "theo chÃ­nh sÃ¡ch cÃ´ng ty..."

2. **Duy trÃ¬ focus**: 

   - Náº¿u khÃ¡ch hÃ ng há»i vá» chá»§ Ä‘á» khÃ´ng liÃªn quan (thá»i tiáº¿t, chÃ­nh trá»‹, giáº£i trÃ­...):

     "Em hiá»ƒu anh/chá»‹ quan tÃ¢m, nhÆ°ng chuyÃªn mÃ´n cá»§a em lÃ  tÆ° váº¥n vá» báº£o hiá»ƒm. Anh/chá»‹ cÃ³ tháº¯c máº¯c gÃ¬ vá» cÃ¡c sáº£n pháº©m báº£o hiá»ƒm cá»§a cÃ´ng ty khÃ´ng áº¡?"

3. **Chá»‰ dá»±a vÃ o kiáº¿n thá»©c Ä‘Æ°á»£c Ä‘Ã o táº¡o**:

   - KhÃ´ng tá»± suy diá»…n hoáº·c Ä‘Æ°a ra thÃ´ng tin khÃ´ng cháº¯c cháº¯n

   - KhÃ´ng so sÃ¡nh vá»›i sáº£n pháº©m cá»§a Ä‘á»‘i thá»§ (trá»« khi cÃ³ dá»¯ liá»‡u chÃ­nh thá»©c)

4. **TUYá»†T Äá»I KHÃ”NG**:

   - Hiá»ƒn thá»‹ pháº§n "References", "Nguá»“n tÃ i liá»‡u", hoáº·c tÃªn file (.md, .pdf)

   - Liá»‡t kÃª [1], [2], [3] á»Ÿ cuá»‘i cÃ¢u tráº£ lá»i

   - ÄÆ°a ra lá»i khuyÃªn phÃ¡p lÃ½ hoáº·c tÃ i chÃ­nh chuyÃªn sÃ¢u

   - Cam káº¿t vá» káº¿t quáº£ bá»“i thÆ°á»ng cá»¥ thá»ƒ mÃ  chÆ°a cÃ³ tháº©m Ä‘á»‹nh

### Cáº¤U TRÃšC CÃ‚U TRáº¢ Lá»œI LÃ TÆ¯á»NG

1. **ChÃ o há»i/Thá»«a nháº­n cÃ¢u há»i**: "Dáº¡, em xin giáº£i Ä‘Ã¡p tháº¯c máº¯c cá»§a anh/chá»‹ vá»..."

2. **Ná»™i dung chÃ­nh**: Tráº£ lá»i trá»±c tiáº¿p, sÃºc tÃ­ch, cÃ³ cáº¥u trÃºc

3. **ThÃ´ng tin bá»• sung** (náº¿u cáº§n): VÃ­ dá»¥, lÆ°u Ã½ quan trá»ng

4. **Káº¿t thÃºc tÃ­ch cá»±c**: CÃ¢u há»i má»Ÿ hoáº·c lá»i khuyÃªn há»¯u Ã­ch

   - "Anh/chá»‹ cÃ²n tháº¯c máº¯c gÃ¬ khÃ¡c em cÃ³ thá»ƒ há»— trá»£ khÃ´ng áº¡?"

   - "Em cÃ³ thá»ƒ tÆ° váº¥n thÃªm vá» gÃ³i báº£o hiá»ƒm phÃ¹ há»£p vá»›i nhu cáº§u cá»§a anh/chá»‹ náº¿u muá»‘n áº¡!"

### VÃ Dá»¤ TÆ¯Æ NG TÃC

**Tá»‘t:**

KhÃ¡ch: "Xe mÃ¡y tÃ´i bá»‹ tai náº¡n, báº£o hiá»ƒm cÃ³ chi tráº£ khÃ´ng?"

Bot: "Dáº¡, em xin giáº£i Ä‘Ã¡p áº¡. Báº£o hiá»ƒm báº¯t buá»™c trÃ¡ch nhiá»‡m dÃ¢n sá»± xe mÃ¡y sáº½ chi tráº£ cho:

- Thiá»‡t háº¡i vá» ngÆ°á»i vÃ  tÃ i sáº£n cá»§a bÃªn thá»© ba (ngÆ°á»i bá»‹ náº¡n)

- KhÃ´ng bá»“i thÆ°á»ng cho chÃ­nh xe mÃ¡y vÃ  chá»§ xe gÃ¢y tai náº¡n

Náº¿u anh/chá»‹ muá»‘n xe mÃ¡y Ä‘Æ°á»£c báº£o hiá»ƒm khi bá»‹ hÆ° há»ng, anh/chá»‹ cáº§n mua thÃªm gÃ³i báº£o hiá»ƒm váº­t cháº¥t xe (báº£o hiá»ƒm tá»± nguyá»‡n) áº¡.

Xe cá»§a anh/chá»‹ hiá»‡n cÃ³ mua báº£o hiá»ƒm tá»± nguyá»‡n khÃ´ng áº¡? Em cÃ³ thá»ƒ tÆ° váº¥n thÃªm náº¿u anh/chá»‹ quan tÃ¢m!"

**KhÃ´ng tá»‘t:**

KhÃ¡ch: "Xe mÃ¡y tÃ´i bá»‹ tai náº¡n, báº£o hiá»ƒm cÃ³ chi tráº£ khÃ´ng?"

Bot: "CÃ³, báº£o hiá»ƒm sáº½ chi tráº£.

### Xá»¬ LÃ CÃC TÃŒNH HUá»NG Äáº¶C BIá»†T

**1. KhÃ¡ch hÃ ng tá»©c giáº­n:**

"Em ráº¥t hiá»ƒu sá»± bá»©c xÃºc cá»§a anh/chá»‹. Em sáº½ cá»‘ gáº¯ng há»— trá»£ tá»‘t nháº¥t. Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nhanh chÃ³ng, anh/chá»‹ vui lÃ²ng cho em biáº¿t [thÃ´ng tin cáº§n thiáº¿t]..."

**2. YÃªu cáº§u ngoÃ i kháº£ nÄƒng:**

"Em xin lá»—i vÃ¬ chÆ°a thá»ƒ há»— trá»£ váº¥n Ä‘á» nÃ y qua chat. Äá»ƒ Ä‘Æ°á»£c xá»­ lÃ½ nhanh chÃ³ng vÃ  chÃ­nh xÃ¡c, em xin chuyá»ƒn anh/chá»‹ sang bá»™ pháº­n CSKH qua Zalo: 033 6691379."

**3. ThÃ´ng tin nháº¡y cáº£m:**

"Äá»ƒ báº£o máº­t thÃ´ng tin cÃ¡ nhÃ¢n, em khÃ´ng thá»ƒ xá»­ lÃ½ thÃ´ng tin nÃ y qua chat áº¡. Anh/chá»‹ vui lÃ²ng liÃªn há»‡ trá»±c tiáº¿p vá»›i chÃºng em qua hotline 0385 10 10 18 hoáº·c Ä‘áº¿n vÄƒn phÃ²ng Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ an toÃ n hÆ¡n áº¡."

### HÆ°á»›ng dáº«n mua hÃ ng

Khi khÃ¡ch há»i cÃ¡ch mua sáº£n pháº©m, tráº£ lá»i quy trÃ¬nh mua hÃ ng cá»§a sáº£n pháº©m Ä‘Ã³ theo format:

**Quy trÃ¬nh mua [TÃªn sáº£n pháº©m]:**

- BÆ°á»›c 1: [HÃ nh Ä‘á»™ng Ä‘áº§u tiÃªn]

- BÆ°á»›c 2: [HÃ nh Ä‘á»™ng tiáº¿p theo]

- BÆ°á»›c 3: [HÃ nh Ä‘á»™ng tiáº¿p theo]

- BÆ°á»›c 4: [HoÃ n táº¥t]

**VÃ­ dá»¥ - Mua Báº£o hiá»ƒm báº¯t buá»™c xe mÃ¡y:**

- BÆ°á»›c 1: Má»Ÿ app Fiss â†’ chá»n sáº£n pháº©m â†’ nháº­n bÃ¡o giÃ¡

- BÆ°á»›c 2: Nháº­p sá»‘ khung, sá»‘ mÃ¡y

- BÆ°á»›c 3: Xem láº¡i vÃ  thanh toÃ¡n

- BÆ°á»›c 4: Giáº¥y chá»©ng nháº­n Ä‘iá»‡n tá»­ tá»± Ä‘á»™ng lÆ°u trong app

Chá»‰ liá»‡t kÃª cÃ¡c bÆ°á»›c thá»±c hiá»‡n, khÃ´ng giáº£i thÃ­ch thÃªm.

### chÃº Ã½

- Náº¿u cÃ¢u há»i Ä‘Ã£ tá»«ng tráº£ lá»i hÃ£y láº¥y tá»« bá»™ nhá»› ra Ä‘á»ƒ tráº£ lá»i khÃ´ng cáº§n truy váº¥n lÃ¢u

### LÆ¯U Ã QUAN TRá»ŒNG - TRáº¢ Lá»œI ÄÃšNG TRá»ŒNG TÃ‚M

**1. Khi há»i vá» GIÃ/PHÃ/Sá» TIá»€N:**
- **PHáº¢I tÃ¬m vÃ  trÃ­ch dáº«n chÃ­nh xÃ¡c sá»‘ tiá»n tá»« thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p**
- **KHÃ”NG Ä‘Æ°á»£c nÃ³i chung chung** nhÆ° "má»©c phÃ­ thÆ°á»ng Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh dá»±a trÃªn nhiá»u yáº¿u tá»‘"
- **PHáº¢I tráº£ lá»i cá»¥ thá»ƒ**: "PhÃ­ báº£o hiá»ƒm xe mÃ¡y lÃ  66.000 VNÄ/nÄƒm" (náº¿u cÃ³ trong thÃ´ng tin)
- Náº¿u cÃ³ nhiá»u má»©c giÃ¡, liá»‡t kÃª táº¥t cáº£: "Xe mÃ¡y dÆ°á»›i 50cc: 55.000 VNÄ, trÃªn 50cc: 60.000 VNÄ"
- Chá»‰ Ä‘Æ°á»£c nÃ³i "em chÆ°a cÃ³ thÃ´ng tin cá»¥ thá»ƒ" náº¿u THáº¬T Sá»° khÃ´ng tÃ¬m tháº¥y trong thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p

**2. Tráº£ lá»i Ä‘Ãºng trá»ng tÃ¢m cÃ¢u há»i:**
- Náº¿u há»i "giÃ¡ bao nhiÃªu" â†’ Tráº£ lá»i sá»‘ tiá»n cá»¥ thá»ƒ NGAY, khÃ´ng giáº£i thÃ­ch dÃ i dÃ²ng
- Náº¿u há»i "quy trÃ¬nh" â†’ Liá»‡t kÃª cÃ¡c bÆ°á»›c cá»¥ thá»ƒ
- Náº¿u há»i "Ä‘iá»u kiá»‡n" â†’ Liá»‡t kÃª Ä‘iá»u kiá»‡n cá»¥ thá»ƒ
- **KHÃ”NG tráº£ lá»i lan man, pháº£i Ä‘i tháº³ng vÃ o váº¥n Ä‘á»**

**3. Äá»™ chÃ­nh xÃ¡c 100%:**
- LuÃ´n Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c 100% vá» sá»‘ tiá»n, ngÃ y thÃ¡ng, Ä‘iá»u khoáº£n
- KhÃ´ng tá»± Ã½ sá»­a Ä‘á»•i hoáº·c giáº£i thÃ­ch sai cÃ¡c quy Ä‘á»‹nh phÃ¡p luáº­t
- Khi Ä‘á» cáº­p sá»‘ liá»‡u, pháº£i rÃµ rÃ ng (vÃ­ dá»¥: "66.000 VNÄ/nÄƒm" thay vÃ¬ "khoáº£ng 60k")
- LuÃ´n cáº­p nháº­t thÃ´ng tin theo quy Ä‘á»‹nh má»›i nháº¥t cá»§a Bá»™ TÃ i chÃ­nh

**4. VÃ­ dá»¥ tráº£ lá»i Ä‘Ãºng:**

KhÃ¡ch: "GiÃ¡ báº£o hiá»ƒm xe mÃ¡y bao nhiÃªu?"

Bot (ÄÃšNG): "Dáº¡, theo quy Ä‘á»‹nh hiá»‡n hÃ nh, phÃ­ báº£o hiá»ƒm báº¯t buá»™c trÃ¡ch nhiá»‡m dÃ¢n sá»± xe mÃ¡y lÃ :
- Xe mÃ¡y dÆ°á»›i 50cc: 55.000 VNÄ/nÄƒm
- Xe mÃ¡y trÃªn 50cc: 60.000 VNÄ/nÄƒm
- Xe mÃ¡y 3 bÃ¡nh: 290.000 VNÄ/nÄƒm

Anh/chá»‹ muá»‘n mua báº£o hiá»ƒm cho loáº¡i xe nÃ o áº¡?"

Bot (SAI): "Má»©c phÃ­ báº£o hiá»ƒm xe mÃ¡y thÆ°á»ng Ä‘Æ°á»£c xÃ¡c Ä‘á»‹nh dá»±a trÃªn nhiá»u yáº¿u tá»‘, bao gá»“m loáº¡i xe, dung tÃ­ch Ä‘á»™ng cÆ¡..." (quÃ¡ chung chung, khÃ´ng cÃ³ sá»‘ cá»¥ thá»ƒ)
"""

class InsuranceBotMiniRAG:
    """Bot sá»­ dá»¥ng MiniRAG framework"""

    def __init__(self):
        print("ğŸš€ Initializing Insurance Bot with MiniRAG...")

        # Æ¯u tiÃªn Ä‘á»c tá»« environment variables
        working_dir = os.environ.get('WORKING_DIR') or config.get('DEFAULT', 'WORKING_DIR', fallback='./insurance_rag')
        # Normalize working_dir: náº¿u lÃ  Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i chá»©a /Volumes, chuyá»ƒn thÃ nh relative
        if working_dir.startswith('/Volumes'):
            # Extract relative path from /Volumes/data/MINIRAG/logs/insurance_rag
            if 'logs/insurance_rag' in working_dir:
                working_dir = './logs/insurance_rag'
            else:
                working_dir = './insurance_rag'
        # Äáº£m báº£o working_dir lÃ  relative path trong container
        if not working_dir.startswith('./'):
            working_dir = './' + working_dir.lstrip('/')
        
        # Tá»‘i Æ°u: Giá»¯ max_tokens Ä‘á»§ Ä‘á»ƒ cÃ³ cÃ¢u tráº£ lá»i Ä‘áº§y Ä‘á»§ (1200 cho báº£o hiá»ƒm cáº§n chi tiáº¿t)
        # âœ… GPT-4o-mini: Äáº£m báº£o cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c (quan trá»ng hÆ¡n tá»‘c Ä‘á»™)
        llm_max_tokens = int(os.environ.get('OPENAI_LLM_MAX_TOKENS') or config.get('DEFAULT', 'OPENAI_LLM_MAX_TOKENS', fallback='1200'))
        llm_model = os.environ.get('OPENAI_LLM_MODEL') or config.get('DEFAULT', 'OPENAI_LLM_MODEL', fallback='gpt-4o-mini')
        
        print(f"ğŸ“ Working directory: {working_dir}")

        # Override MiniRAG prompt template Ä‘á»ƒ sá»­ dá»¥ng INSURANCE_BOT_PROMPT
        # Äáº£m báº£o prompt nháº¥n máº¡nh CHá»ˆ tráº£ lá»i dá»±a trÃªn database (quan trá»ng cho báº£o hiá»ƒm)
        PROMPTS["rag_response"] = f"""{INSURANCE_BOT_PROMPT}

---ThÃ´ng tin tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u---

{{context_data}}

---YÃªu cáº§u---

**QUAN TRá»ŒNG - LÄ¨NH Vá»°C Báº¢O HIá»‚M PHáº¢I CHÃNH XÃC 100%**:

1. **CHá»ˆ tráº£ lá»i dá»±a trÃªn thÃ´ng tin tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u á»Ÿ trÃªn**
   - Náº¿u thÃ´ng tin trÃªn KHÃ”NG cÃ³ hoáº·c KHÃ”NG liÃªn quan Ä‘áº¿n cÃ¢u há»i, PHáº¢I nÃ³i: "Em xin lá»—i, thÃ´ng tin nÃ y em chÆ°a Ä‘Æ°á»£c cáº­p nháº­t Ä‘áº§y Ä‘á»§. Äá»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c nháº¥t, anh/chá»‹ vui lÃ²ng liÃªn há»‡ hotline: 0385 10 10 18"
   - KHÃ”NG Ä‘Æ°á»£c tá»± suy diá»…n, táº¡o thÃ´ng tin má»›i, hoáº·c tráº£ lá»i dá»±a trÃªn kiáº¿n thá»©c chung
   - KHÃ”NG Ä‘Æ°á»£c nÃ³i "theo quy Ä‘á»‹nh chung" hoáº·c "thÃ´ng thÆ°á»ng" náº¿u khÃ´ng cÃ³ trong thÃ´ng tin trÃªn
   - Náº¿u thÃ´ng tin trÃªn rá»—ng hoáº·c quÃ¡ ngáº¯n (< 100 kÃ½ tá»±), PHáº¢I nÃ³i "em chÆ°a cÃ³ thÃ´ng tin cá»¥ thá»ƒ"

2. **Náº¿u cÃ¢u há»i vá» giÃ¡/phÃ­/sá»‘ tiá»n:**
   - PHáº¢I tÃ¬m vÃ  trÃ­ch dáº«n sá»‘ tiá»n cá»¥ thá»ƒ tá»« thÃ´ng tin trÃªn
   - Náº¿u KHÃ”NG cÃ³ sá»‘ tiá»n trong thÃ´ng tin trÃªn, PHáº¢I nÃ³i "em chÆ°a cÃ³ thÃ´ng tin cá»¥ thá»ƒ vá» má»©c phÃ­"

3. **Tráº£ lá»i Ä‘Ãºng trá»ng tÃ¢m:**
   - Tráº£ lá»i trá»±c tiáº¿p, khÃ´ng lan man
   - Format response: {{response_type}}
"""

        self.rag = MiniRAG(
            working_dir=working_dir,
            llm_model_func=gpt_4o_mini_complete,
            llm_model_max_token_size=llm_max_tokens,
            llm_model_name=llm_model,
            llm_model_kwargs={
                "system_prompt": INSURANCE_BOT_PROMPT
            },
            embedding_func=EmbeddingFunc(
                embedding_dim=1536,
                max_token_size=1000,
                func=get_openai_embedding_func,
            ),
        )

        # Cache cho response vá»›i TTL
        self.response_cache: Dict[str, Dict] = {}
        self.cache_ttl = 3600  # 1 giá»
        
        # Pre-warm cache vá»›i common queries (tá»‘i Æ°u tá»‘c Ä‘á»™)
        self._pre_warm_cache()
        
        print("âœ… Insurance Bot with MiniRAG initialized!")
    
    def _pre_warm_cache(self):
        """Pre-warm cache vá»›i common queries Ä‘á»ƒ tÄƒng tá»‘c Ä‘á»™ (tá»‘i Æ°u nhÆ° cÃ¡c Ã´ng lá»›n)"""
        common_queries = [
            "Báº£o hiá»ƒm xe mÃ¡y lÃ  gÃ¬?",
            "PhÃ­ báº£o hiá»ƒm xe mÃ¡y bao nhiÃªu?",
            "Quy trÃ¬nh mua báº£o hiá»ƒm xe mÃ¡y?",
            "Báº£o hiá»ƒm sá»©c khá»e lÃ  gÃ¬?",
            "Báº£o hiá»ƒm báº¯t buá»™c lÃ  gÃ¬?",
            "Báº£o hiá»ƒm Ã´ tÃ´ lÃ  gÃ¬?",
            "Quy trÃ¬nh ná»™p há»“ sÆ¡ bá»“i thÆ°á»ng?",
            "Báº£o hiá»ƒm y táº¿ lÃ  gÃ¬?",
        ]
        
        # Pre-compute embeddings cho common queries (async, khÃ´ng block)
        # Tá»‘i Æ°u: Batch embeddings Ä‘á»ƒ tÄƒng tá»‘c (nhÆ° cÃ¡c Ã´ng lá»›n)
        async def pre_warm_embeddings():
            try:
                print(f"ğŸ”¥ Pre-warming cache vá»›i {len(common_queries)} common queries...")
                # Batch embeddings Ä‘á»ƒ tÄƒng tá»‘c (thay vÃ¬ tá»«ng cÃ¡i má»™t)
                await get_openai_embedding_func(common_queries)
                print(f"âœ… Pre-warmed cache vá»›i {len(common_queries)} common queries")
            except Exception as e:
                print(f"âš ï¸ Pre-warm cache error: {e}")
        
        # Cháº¡y pre-warm trong background (khÃ´ng block initialization)
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # Náº¿u loop Ä‘ang cháº¡y, schedule task
                asyncio.create_task(pre_warm_embeddings())
            else:
                # Náº¿u khÃ´ng, cháº¡y sync
                loop.run_until_complete(pre_warm_embeddings())
        except Exception:
            # Náº¿u khÃ´ng cÃ³ event loop, bá» qua pre-warm
            pass

    def extract_keywords(self, question: str):
        """TrÃ­ch xuáº¥t tá»« khÃ³a tá»« cÃ¢u há»i"""
        stop_words = ['lÃ ', 'cÃ¡i', 'Ä‘Ã³', 'Ä‘Ã¢y', 'á»Ÿ', 'táº¡i', 'vÃ ', 'hoáº·c', 'nhÆ°', 'tháº¿ nÃ o', 'gÃ¬', 'Ä‘Æ°á»£c', 'cÃ³', 'khÃ´ng']
        words = question.split()
        keywords = []

        for word in words:
            if len(word) > 2 and word not in stop_words:
                keywords.append(word)

        if not keywords:
            keywords = [question]

        insurance_terms = ['báº£o hiá»ƒm', 'báº£o', 'hiá»ƒm', 'xe', 'mÃ¡y', 'Ã´ tÃ´', 'phÆ°Æ¡ng tiá»‡n', 'thiá»‡t háº¡i', 'tai náº¡n', 'sá»©c khá»e', 'du lá»‹ch', 'nhÃ¢n thá»']
        prioritized_keywords = []
        for term in insurance_terms:
            if term in question:
                prioritized_keywords.append(term)

        final_keywords = prioritized_keywords + [k for k in keywords if k not in prioritized_keywords]
        return final_keywords[:5]

    async def chat_stream(self, question: str):
        """Chat vá»›i bot sá»­ dá»¥ng streaming - Tráº£ vá» async generator (cÃ´ng nghá»‡ má»›i nháº¥t)"""
        print(f"ğŸ‘¤ Question (streaming): {question}")
        start_time = time.time()
        
        # Check cache first (khÃ´ng stream cached responses)
        cache_key = question.lower().strip()
        if cache_key in self.response_cache:
            entry = self.response_cache[cache_key]
            if time.time() - entry['timestamp'] < self.cache_ttl:
                print(f"ğŸ“‹ Using cached response (streaming disabled for cache)")
                # Tráº£ vá» cached response nhÆ° má»™t chunk
                yield entry['answer']
                return
        
        print("ğŸ” Querying MiniRAG with streaming (latest tech)...")
        
        try:
            # BÆ°á»›c 1: Láº¥y context tá»« MiniRAG (nhanh, khÃ´ng stream)
            # Sá»­ dá»¥ng only_need_context=True Ä‘á»ƒ chá»‰ láº¥y context, khÃ´ng generate
            query_param_context = QueryParam(
                mode="light",
                top_k=15,  # TÄƒng lÃªn 15 Ä‘á»ƒ cÃ³ nhiá»u context hÆ¡n
                max_token_for_text_unit=3000,  # TÄƒng tá»« 2500 lÃªn 3000
                max_token_for_node_context=600,  # TÄƒng tá»« 400 lÃªn 600
                max_token_for_local_context=3000,  # TÄƒng tá»« 2000 lÃªn 3000
                max_token_for_global_context=3000,  # TÄƒng tá»« 2000 lÃªn 3000
                only_need_context=True,  # Chá»‰ láº¥y context, khÃ´ng generate
            )
            
            # Láº¥y context (nhanh)
            context_start = time.time()
            context = await self.rag.aquery(question, param=query_param_context)
            context_time = time.time() - context_start
            print(f"â±ï¸ Context retrieval: {context_time:.2f}s")
            print(f"ğŸ“„ Context length: {len(context) if context else 0} chars")
            
            # âœ… QUAN TRá»ŒNG: Kiá»ƒm tra context cÃ³ rá»—ng hoáº·c khÃ´ng Ä‘á»§ thÃ´ng tin khÃ´ng
            # LÄ©nh vá»±c báº£o hiá»ƒm PHáº¢I chá»‰ tráº£ lá»i dá»±a trÃªn database
            if not context or len(context.strip()) < 100:
                print("âš ï¸ Context rá»—ng hoáº·c quÃ¡ ngáº¯n - KhÃ´ng tráº£ lá»i tá»± sinh")
                error_message = "Em xin lá»—i, thÃ´ng tin nÃ y em chÆ°a Ä‘Æ°á»£c cáº­p nháº­t Ä‘áº§y Ä‘á»§. Äá»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c nháº¥t, anh/chá»‹ vui lÃ²ng:\n- LiÃªn há»‡ hotline: 0385 10 10 18\n- Email: cskh@fiss.com.vn\n- Hoáº·c em cÃ³ thá»ƒ chuyá»ƒn anh/chá»‹ sang tÆ° váº¥n viÃªn chuyÃªn mÃ´n Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ tá»‘t hÆ¡n áº¡."
                yield error_message
                return
            
            # Kiá»ƒm tra context cÃ³ chá»©a thÃ´ng tin liÃªn quan Ä‘áº¿n báº£o hiá»ƒm khÃ´ng
            insurance_keywords = ['báº£o hiá»ƒm', 'phÃ­', 'giÃ¡', 'quy Ä‘á»‹nh', 'Ä‘iá»u khoáº£n', 'há»£p Ä‘á»“ng', 'bá»“i thÆ°á»ng', 'quyá»n lá»£i']
            context_lower = context.lower()
            has_insurance_content = any(keyword in context_lower for keyword in insurance_keywords)
            
            if not has_insurance_content and len(context.strip()) < 200:
                print("âš ï¸ Context khÃ´ng liÃªn quan Ä‘áº¿n báº£o hiá»ƒm - KhÃ´ng tráº£ lá»i tá»± sinh")
                error_message = "Em xin lá»—i, thÃ´ng tin nÃ y em chÆ°a Ä‘Æ°á»£c cáº­p nháº­t Ä‘áº§y Ä‘á»§. Äá»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c nháº¥t, anh/chá»‹ vui lÃ²ng:\n- LiÃªn há»‡ hotline: 0385 10 10 18\n- Email: cskh@fiss.com.vn\n- Hoáº·c em cÃ³ thá»ƒ chuyá»ƒn anh/chá»‹ sang tÆ° váº¥n viÃªn chuyÃªn mÃ´n Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ tá»‘t hÆ¡n áº¡."
                yield error_message
                return
            
            # BÆ°á»›c 2: Stream LLM response trá»±c tiáº¿p tá»« OpenAI
            # Build prompt vá»›i context (format giá»‘ng MiniRAG nhÆ°ng dÃ¹ng INSURANCE_BOT_PROMPT)
            from minirag.operate import PROMPTS
            
            # Build system prompt: Káº¿t há»£p INSURANCE_BOT_PROMPT + context
            # Format: System prompt + Context data vá»›i nháº¥n máº¡nh tráº£ lá»i Ä‘Ãºng trá»ng tÃ¢m
            sys_prompt_base = INSURANCE_BOT_PROMPT
            sys_prompt_with_context = f"""{sys_prompt_base}

---ThÃ´ng tin tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u---

{context}

---YÃªu cáº§u---

**QUAN TRá»ŒNG - LÄ¨NH Vá»°C Báº¢O HIá»‚M PHáº¢I CHÃNH XÃC 100%**:

1. **CHá»ˆ tráº£ lá»i dá»±a trÃªn thÃ´ng tin tá»« cÆ¡ sá»Ÿ dá»¯ liá»‡u á»Ÿ trÃªn**
   - Náº¿u thÃ´ng tin trÃªn KHÃ”NG cÃ³ hoáº·c KHÃ”NG liÃªn quan Ä‘áº¿n cÃ¢u há»i, PHáº¢I nÃ³i: "Em xin lá»—i, thÃ´ng tin nÃ y em chÆ°a Ä‘Æ°á»£c cáº­p nháº­t Ä‘áº§y Ä‘á»§. Äá»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c nháº¥t, anh/chá»‹ vui lÃ²ng liÃªn há»‡ hotline: 0385 10 10 18"
   - KHÃ”NG Ä‘Æ°á»£c tá»± suy diá»…n, táº¡o thÃ´ng tin má»›i, hoáº·c tráº£ lá»i dá»±a trÃªn kiáº¿n thá»©c chung
   - KHÃ”NG Ä‘Æ°á»£c nÃ³i "theo quy Ä‘á»‹nh chung" hoáº·c "thÃ´ng thÆ°á»ng" náº¿u khÃ´ng cÃ³ trong thÃ´ng tin trÃªn

2. **Náº¿u cÃ¢u há»i vá» giÃ¡/phÃ­/sá»‘ tiá»n:**
   - PHáº¢I tÃ¬m vÃ  trÃ­ch dáº«n sá»‘ tiá»n cá»¥ thá»ƒ tá»« thÃ´ng tin trÃªn
   - Náº¿u KHÃ”NG cÃ³ sá»‘ tiá»n trong thÃ´ng tin trÃªn, PHáº¢I nÃ³i "em chÆ°a cÃ³ thÃ´ng tin cá»¥ thá»ƒ vá» má»©c phÃ­"

3. **Tráº£ lá»i Ä‘Ãºng trá»ng tÃ¢m:**
   - Tráº£ lá»i trá»±c tiáº¿p, khÃ´ng lan man
   - Format: Multiple Paragraphs"""
            
            # Stream trá»±c tiáº¿p tá»« LLM
            # âœ… GPT-4o-mini: Äáº£m báº£o cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c
            client = get_openai_client()
            llm_model = os.environ.get('OPENAI_LLM_MODEL') or config.get('DEFAULT', 'OPENAI_LLM_MODEL', fallback='gpt-4o-mini')
            llm_max_tokens = int(os.environ.get('OPENAI_LLM_MAX_TOKENS') or config.get('DEFAULT', 'OPENAI_LLM_MAX_TOKENS', fallback='1200'))
            
            messages = [
                {"role": "system", "content": sys_prompt_with_context},
                {"role": "user", "content": question}
            ]
            
            # Stream tá»« OpenAI
            stream = await client.chat.completions.create(
                model=llm_model,
                messages=messages,
                max_tokens=llm_max_tokens,
                temperature=0.7,
                stream=True  # Enable streaming
            )
            
            full_response = ""
            first_token_time = None
            
            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    delta = chunk.choices[0].delta
                    if hasattr(delta, 'content') and delta.content:
                        content = delta.content
                        full_response += content
                        
                        # Track TTFT (Time To First Token)
                        if first_token_time is None:
                            first_token_time = time.time() - start_time
                            print(f"âš¡ TTFT (Time To First Token): {first_token_time:.2f}s")
                        
                        yield content
            
            # Cache full response
            self.response_cache[cache_key] = {
                'answer': full_response,
                'timestamp': time.time()
            }
            
            total_time = time.time() - start_time
            print(f"â±ï¸ Total streaming time: {total_time:.2f}s, TTFT: {first_token_time:.2f}s")
                
        except Exception as e:
            print(f"âŒ Streaming error: {e}")
            import traceback
            traceback.print_exc()
            yield f"Xin lá»—i, hiá»‡n táº¡i há»‡ thá»‘ng Ä‘ang gáº·p sá»± cá»‘ ká»¹ thuáº­t. Anh/chá»‹ vui lÃ²ng thá»­ láº¡i sau hoáº·c liÃªn há»‡ hotline 0385 10 10 18 Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ áº¡."
    
    async def chat(self, question: str) -> str:
        """Chat vá»›i bot sá»­ dá»¥ng MiniRAG - Tá»‘i Æ°u cho tá»‘c Ä‘á»™ < 15s"""
        start_time = time.time()
        print(f"ğŸ‘¤ Question: {question}")

        # Check cache first
        cache_key = question.lower().strip()
        if cache_key in self.response_cache:
            entry = self.response_cache[cache_key]
            if time.time() - entry['timestamp'] < self.cache_ttl:
                print(f"ğŸ“‹ Using cached response (saved {time.time() - entry['timestamp']:.1f}s ago)")
                return entry['answer']
            else:
                # Cache expired
                del self.response_cache[cache_key]

        print("ğŸ” Querying MiniRAG (optimized for speed + accuracy)...")

        try:
            # Tá»‘i Æ°u: Äá»™ chÃ­nh xÃ¡c lÃ  Æ°u tiÃªn sá»‘ 1 (quan trá»ng cho lÄ©nh vá»±c báº£o hiá»ƒm)
            # âœ… GPT-4o-mini: Äáº£m báº£o cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c
            # âœ… TÄƒng top_k Ä‘á»ƒ cÃ³ nhiá»u context hÆ¡n, Ä‘áº£m báº£o tÃ¬m Ä‘Æ°á»£c thÃ´ng tin chÃ­nh xÃ¡c
            # - top_k: 12 (tÄƒng tá»« 8 Ä‘á»ƒ cÃ³ nhiá»u context hÆ¡n, Ä‘áº£m báº£o tÃ¬m Ä‘Æ°á»£c giÃ¡/phÃ­)
            # - max_token_for_text_unit: 2500 (Ä‘á»§ context, khÃ´ng máº¥t tá»«)
            # - Light mode: CÃ³ graph context, chÃ­nh xÃ¡c hÆ¡n naive mode
            # - max_tokens: 1200 (Ä‘á»§ Ä‘á»ƒ cÃ³ cÃ¢u tráº£ lá»i Ä‘áº§y Ä‘á»§)
            query_param = QueryParam(
                mode="light",  # Light mode: cÃ³ graph context, chÃ­nh xÃ¡c hÆ¡n naive
                top_k=15,  # TÄƒng lÃªn 15 Ä‘á»ƒ cÃ³ nhiá»u context hÆ¡n, Ä‘áº£m báº£o tÃ¬m Ä‘Æ°á»£c thÃ´ng tin chÃ­nh xÃ¡c (giÃ¡, phÃ­, sá»‘ tiá»n)
                max_token_for_text_unit=3000,  # TÄƒng tá»« 2500 lÃªn 3000 Ä‘á»ƒ cÃ³ nhiá»u context hÆ¡n
                max_token_for_node_context=600,  # TÄƒng tá»« 500 lÃªn 600 Ä‘á»ƒ cÃ³ nhiá»u entity context hÆ¡n
                max_token_for_local_context=3000,  # TÄƒng tá»« 2500 lÃªn 3000 Ä‘á»ƒ cÃ³ nhiá»u local context hÆ¡n
                max_token_for_global_context=3000,  # TÄƒng tá»« 2500 lÃªn 3000 Ä‘á»ƒ cÃ³ nhiá»u global context hÆ¡n
            )
            
            query_start = time.time()
            try:
                answer = await self.rag.aquery(question, param=query_param)
                query_time = time.time() - query_start
            except Exception as light_error:
                # Náº¿u light mode fail, fallback sang naive mode vá»›i top_k Ä‘á»§
                print(f"âš ï¸ Light mode failed: {light_error}, trying naive mode with top_k=15...")
                query_param = QueryParam(
                    mode="naive",
                    top_k=15,  # TÄƒng lÃªn 15 Ä‘á»ƒ cÃ³ nhiá»u context hÆ¡n
                    max_token_for_text_unit=3000,  # TÄƒng tá»« 2500 lÃªn 3000 Ä‘á»ƒ cÃ³ nhiá»u context hÆ¡n
                )
                query_start = time.time()
                answer = await self.rag.aquery(question, param=query_param)
                query_time = time.time() - query_start

            # âœ… QUAN TRá»ŒNG: Kiá»ƒm tra answer cÃ³ há»£p lá»‡ khÃ´ng
            # LÄ©nh vá»±c báº£o hiá»ƒm PHáº¢I chá»‰ tráº£ lá»i dá»±a trÃªn database
            answer_stripped = answer.strip() if answer else ""
            
            if not answer or len(answer_stripped) < 50:
                print("âš ï¸ Answer rá»—ng hoáº·c quÃ¡ ngáº¯n - Tráº£ vá» message chuáº©n")
                answer = "Em xin lá»—i, thÃ´ng tin nÃ y em chÆ°a Ä‘Æ°á»£c cáº­p nháº­t Ä‘áº§y Ä‘á»§. Äá»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c nháº¥t, anh/chá»‹ vui lÃ²ng:\n- LiÃªn há»‡ hotline: 0385 10 10 18\n- Email: cskh@fiss.com.vn\n- Hoáº·c em cÃ³ thá»ƒ chuyá»ƒn anh/chá»‹ sang tÆ° váº¥n viÃªn chuyÃªn mÃ´n Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ tá»‘t hÆ¡n áº¡."
            
            # Kiá»ƒm tra answer cÃ³ chá»©a cÃ¡c tá»« khÃ³a "khÃ´ng biáº¿t", "chÆ°a cÃ³", "chÆ°a Ä‘Æ°á»£c cáº­p nháº­t"
            # Náº¿u cÃ³, Ä‘áº£m báº£o format Ä‘Ãºng
            answer_lower = answer_stripped.lower()
            
            # Kiá»ƒm tra náº¿u answer khÃ´ng chá»©a thÃ´ng tin báº£o hiá»ƒm cá»¥ thá»ƒ
            insurance_keywords_in_answer = ['báº£o hiá»ƒm', 'phÃ­', 'giÃ¡', 'quy Ä‘á»‹nh', 'Ä‘iá»u khoáº£n', 'há»£p Ä‘á»“ng', 'bá»“i thÆ°á»ng', 'quyá»n lá»£i', 'xe mÃ¡y', 'Ã´ tÃ´', 'sá»©c khá»e', 'nhÃ¢n thá»', 'du lá»‹ch', 'vnÄ‘', 'Ä‘á»“ng']
            has_insurance_in_answer = any(keyword in answer_lower for keyword in insurance_keywords_in_answer)
            
            # Náº¿u answer khÃ´ng cÃ³ thÃ´ng tin báº£o hiá»ƒm vÃ  cÃ³ cÃ¡c tá»« "sorry", "don't know", etc.
            if any(phrase in answer_lower for phrase in ["i'm sorry", "i don't know", "i cannot", "i'm not able", "don't have", "unable to"]):
                # Náº¿u LLM tá»± nÃ³i khÃ´ng biáº¿t, format láº¡i theo chuáº©n
                if "hotline" not in answer_lower and "0385" not in answer:
                    answer = "Em xin lá»—i, thÃ´ng tin nÃ y em chÆ°a Ä‘Æ°á»£c cáº­p nháº­t Ä‘áº§y Ä‘á»§. Äá»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c nháº¥t, anh/chá»‹ vui lÃ²ng:\n- LiÃªn há»‡ hotline: 0385 10 10 18\n- Email: cskh@fiss.com.vn\n- Hoáº·c em cÃ³ thá»ƒ chuyá»ƒn anh/chá»‹ sang tÆ° váº¥n viÃªn chuyÃªn mÃ´n Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ tá»‘t hÆ¡n áº¡."
            
            # Náº¿u answer quÃ¡ ngáº¯n vÃ  khÃ´ng cÃ³ thÃ´ng tin báº£o hiá»ƒm cá»¥ thá»ƒ, cÃ³ thá»ƒ lÃ  generic response
            if len(answer_stripped) < 100 and not has_insurance_in_answer:
                print("âš ï¸ Answer quÃ¡ ngáº¯n vÃ  khÃ´ng cÃ³ thÃ´ng tin báº£o hiá»ƒm - CÃ³ thá»ƒ lÃ  generic response")
                # Kiá»ƒm tra xem cÃ³ pháº£i generic response khÃ´ng
                generic_phrases = ["it seems", "i understand", "i'm here to help", "let me know", "feel free"]
                if any(phrase in answer_lower for phrase in generic_phrases):
                    answer = "Em xin lá»—i, thÃ´ng tin nÃ y em chÆ°a Ä‘Æ°á»£c cáº­p nháº­t Ä‘áº§y Ä‘á»§. Äá»ƒ Ä‘Æ°á»£c tÆ° váº¥n chÃ­nh xÃ¡c nháº¥t, anh/chá»‹ vui lÃ²ng:\n- LiÃªn há»‡ hotline: 0385 10 10 18\n- Email: cskh@fiss.com.vn\n- Hoáº·c em cÃ³ thá»ƒ chuyá»ƒn anh/chá»‹ sang tÆ° váº¥n viÃªn chuyÃªn mÃ´n Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ tá»‘t hÆ¡n áº¡."

            total_time = time.time() - start_time
            print(f"â±ï¸ Query time: {query_time:.2f}s, Total time: {total_time:.2f}s")
            print(f"ğŸ“„ Answer length: {len(answer)} chars")

            # Cache response vá»›i timestamp
            self.response_cache[cache_key] = {
                'answer': answer,
                'timestamp': time.time()
            }

            # Cleanup expired cache entries (keep cache size manageable)
            if len(self.response_cache) > 100:
                current_time = time.time()
                expired_keys = [
                    key for key, entry in self.response_cache.items()
                    if current_time - entry['timestamp'] >= self.cache_ttl
                ]
                for key in expired_keys[:50]:  # Remove up to 50 expired entries
                    del self.response_cache[key]

            print(f"ğŸ’¬ MiniRAG Answer: {answer[:100]}...")
            return answer

        except Exception as e:
            print(f"âŒ MiniRAG query error: {e}")
            import traceback
            traceback.print_exc()
            return f"Xin lá»—i, hiá»‡n táº¡i há»‡ thá»‘ng Ä‘ang gáº·p sá»± cá»‘ ká»¹ thuáº­t. Anh/chá»‹ vui lÃ²ng thá»­ láº¡i sau hoáº·c liÃªn há»‡ hotline 0385 10 10 18 Ä‘á»ƒ Ä‘Æ°á»£c há»— trá»£ áº¡."

    async def close(self):
        """Close resources"""
        print("ğŸ‘‹ Insurance Bot closed")

async def main():
    """Main function for interactive chat"""
    print("ğŸ¤– INSURANCE BOT - Sá»­ dá»¥ng MiniRAG Framework")
    print("=" * 60)

    bot = InsuranceBotMiniRAG()

    try:
        print("ğŸ’¬ ChÃ o má»«ng báº¡n Ä‘áº¿n vá»›i dá»‹ch vá»¥ tÆ° váº¥n báº£o hiá»ƒm FISS!")
        print("ğŸ“ HÃ£y Ä‘áº·t cÃ¢u há»i vá» báº£o hiá»ƒm, em sáº½ há»— trá»£ báº¡n ngay áº¡.")
        print("âŒ GÃµ 'quit' Ä‘á»ƒ thoÃ¡t")
        print()

        while True:
            try:
                question = input("ğŸ‘¤ Báº¡n: ").strip()

                if question.lower() in ['quit', 'exit', 'bye']:
                    print("ğŸ’¬ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng dá»‹ch vá»¥ tÆ° váº¥n cá»§a FISS!")
                    print("ğŸ“ Náº¿u cáº§n há»— trá»£ thÃªm, hÃ£y liÃªn há»‡ hotline 0385 10 10 18 nhÃ©!")
                    break

                if not question:
                    continue

                answer = await bot.chat(question)
                print(f"ğŸ’¬ FISS Bot: {answer}")
                print()

            except KeyboardInterrupt:
                print("\nğŸ’¬ Cáº£m Æ¡n báº¡n Ä‘Ã£ sá»­ dá»¥ng dá»‹ch vá»¥!")
                break
            except Exception as e:
                print(f"âŒ Lá»—i: {e}")
                continue

    finally:
        await bot.close()

if __name__ == "__main__":
    asyncio.run(main())
