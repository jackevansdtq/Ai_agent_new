# üöÄ Advanced Performance Optimization Guide

## M·ª•c ti√™u
Gi·∫£m th·ªùi gian x·ª≠ l√Ω t·ª´ **30 gi√¢y** xu·ªëng **d∆∞·ªõi 16 gi√¢y** b·∫±ng c√°c k·ªπ thu·∫≠t t·ª´ c√°c c√¥ng ty l·ªõn.

## C√°c t·ªëi ∆∞u ƒë√£ implement

### 1. ‚úÖ Singleton OpenAI Client (Connection Pooling)
- **V·∫•n ƒë·ªÅ**: T·∫°o client m·ªõi m·ªói request ‚Üí overhead connection setup
- **Gi·∫£i ph√°p**: Singleton pattern ƒë·ªÉ reuse connection
- **L·ª£i √≠ch**: Gi·∫£m ~2-5s overhead m·ªói request

```python
_openai_client: Optional[AsyncOpenAI] = None

def get_openai_client() -> AsyncOpenAI:
    global _openai_client
    if _openai_client is None:
        _openai_client = AsyncOpenAI(
            api_key=api_key,
            base_url=base_url,
            timeout=30.0,  # Timeout ng·∫Øn h∆°n
            max_retries=2,  # Fail fast
        )
    return _openai_client
```

### 2. ‚úÖ Event Loop Reuse
- **V·∫•n ƒë·ªÅ**: T·∫°o event loop m·ªõi m·ªói request ‚Üí overhead
- **Gi·∫£i ph√°p**: Reuse global event loop
- **L·ª£i √≠ch**: Gi·∫£m ~1-2s overhead

```python
_global_event_loop: Optional[asyncio.AbstractEventLoop] = None

def get_or_create_event_loop():
    global _global_event_loop
    if _global_event_loop is None or _global_event_loop.is_closed():
        _global_event_loop = asyncio.new_event_loop()
        asyncio.set_event_loop(_global_event_loop)
    return _global_event_loop
```

### 3. ‚úÖ Query Parameters T·ªëi ∆∞u
- **top_k**: 3 (gi·∫£m t·ª´ 5)
- **max_token_for_text_unit**: 1000 (gi·∫£m t·ª´ 1500)
- **llm_max_tokens**: 800 (gi·∫£m t·ª´ 1000)
- **Mode**: Naive (nhanh nh·∫•t)

### 4. ‚úÖ Caching Strategy
- Response cache v·ªõi TTL 1 gi·ªù
- Embedding cache ƒë·ªÉ tr√°nh g·ªçi API l·∫∑p l·∫°i
- Auto cleanup expired entries

## C√°c t·ªëi ∆∞u ti·∫øp theo (ch∆∞a implement)

### A. Response Streaming ‚è≥
**M·ª•c ti√™u**: Gi·∫£m Time To First Token (TTFT)

```python
# Thay v√¨ ch·ªù to√†n b·ªô response:
answer = await self.rag.aquery(question, param=query_param)

# Stream response:
async for chunk in self.rag.aquery_stream(question, param=query_param):
    yield chunk  # Tr·∫£ v·ªÅ ngay khi c√≥ token ƒë·∫ßu ti√™n
```

**L·ª£i √≠ch**: 
- User th·∫•y response ngay (perceived latency gi·∫£m)
- TTFT: ~2-3s thay v√¨ 15-30s

### B. Parallel Processing ‚è≥
**M·ª•c ti√™u**: Ch·∫°y song song c√°c operations ƒë·ªôc l·∫≠p

```python
# Sequential (hi·ªán t·∫°i):
embedding = await get_embedding(query)
results = await vector_search(embedding)
answer = await llm_generate(results)

# Parallel:
embedding_task = get_embedding(query)
# Trong khi ch·ªù embedding, c√≥ th·ªÉ pre-fetch common data
embedding = await embedding_task
results = await vector_search(embedding)
answer = await llm_generate(results)
```

**L·ª£i √≠ch**: Gi·∫£m ~3-5s

### C. Pre-computation & Pre-warming ‚è≥
**M·ª•c ti√™u**: Pre-compute embeddings cho common queries

```python
# Pre-warm cache v·ªõi popular questions
COMMON_QUERIES = [
    "B·∫£o hi·ªÉm xe m√°y l√† g√¨?",
    "Ph√≠ b·∫£o hi·ªÉm xe m√°y bao nhi√™u?",
    "Quy tr√¨nh mua b·∫£o hi·ªÉm xe m√°y?",
]

async def pre_warm_cache():
    for query in COMMON_QUERIES:
        await get_openai_embedding_func([query])
```

**L·ª£i √≠ch**: Cache hit rate tƒÉng ‚Üí response time gi·∫£m

### D. Vector Database Optimization ‚è≥
**M·ª•c ti√™u**: T·ªëi ∆∞u vector search

1. **Index optimization**: ƒê·∫£m b·∫£o vector index ƒë∆∞·ª£c optimize
2. **Batch queries**: Batch multiple queries n·∫øu c√≥ th·ªÉ
3. **Approximate search**: S·ª≠ d·ª•ng approximate nearest neighbor (ANN) thay v√¨ exact

### E. LLM Optimization ‚è≥
**M·ª•c ti√™u**: T·ªëi ∆∞u LLM generation

1. **Temperature**: Gi·∫£m temperature ƒë·ªÉ generation nhanh h∆°n
2. **Stop sequences**: Th√™m stop sequences ƒë·ªÉ d·ª´ng s·ªõm
3. **Streaming**: Stream tokens thay v√¨ ch·ªù to√†n b·ªô

### F. Database Connection Pooling ‚è≥
**M·ª•c ti√™u**: T·ªëi ∆∞u Neo4J connections

```python
# Neo4J connection pool
neo4j_driver = GraphDatabase.driver(
    uri,
    auth=(username, password),
    max_connection_lifetime=3600,
    max_connection_pool_size=50,  # TƒÉng pool size
    connection_acquisition_timeout=5,
)
```

## Best Practices t·ª´ c√°c c√¥ng ty l·ªõn

### 1. OpenAI ChatGPT
- **Streaming responses**: Lu√¥n stream ƒë·ªÉ gi·∫£m perceived latency
- **Connection pooling**: Reuse HTTP connections
- **Timeout management**: Fail fast v·ªõi timeout ng·∫Øn
- **Retry logic**: Exponential backoff v·ªõi max retries

### 2. Anthropic Claude
- **Pre-computation**: Pre-compute embeddings cho common queries
- **Caching**: Aggressive caching strategy
- **Parallel processing**: Parallelize independent operations

### 3. Google Bard/Gemini
- **Approximate search**: S·ª≠ d·ª•ng ANN cho vector search
- **Batch processing**: Batch multiple requests
- **Edge caching**: Cache ·ªü edge locations

## Monitoring & Metrics

### Key Metrics:
1. **Query time**: Th·ªùi gian MiniRAG query (m·ª•c ti√™u: < 10s)
2. **Total processing time**: T·ªïng th·ªùi gian t·ª´ request ƒë·∫øn response (m·ª•c ti√™u: < 16s)
3. **TTFT (Time To First Token)**: Th·ªùi gian ƒë·∫øn token ƒë·∫ßu ti√™n (m·ª•c ti√™u: < 3s)
4. **Cache hit rate**: T·ª∑ l·ªá cache hits (m·ª•c ti√™u: > 50%)
5. **API call count**: S·ªë l∆∞·ª£ng API calls (m·ª•c ti√™u: minimize)

### Logging:
```bash
# Monitor performance
docker-compose logs insurance-bot | grep -E "(Query time|Total time|Cache hit|API call)"

# Track bottlenecks
docker-compose logs insurance-bot | grep -E "(Fetching|HTTP Request|timeout)"
```

## Implementation Priority

### Phase 1 (ƒê√£ l√†m) ‚úÖ
1. Singleton OpenAI client
2. Event loop reuse
3. Query parameters optimization
4. Caching improvements

### Phase 2 (∆Øu ti√™n cao) üî•
1. Response streaming
2. Parallel processing
3. Pre-warming cache

### Phase 3 (∆Øu ti√™n trung b√¨nh) üìã
1. Vector database optimization
2. LLM optimization
3. Database connection pooling

## Expected Results

### Current (sau Phase 1):
- Processing time: **~29s**
- Query time: **~15-20s**

### After Phase 2:
- Processing time: **~12-15s** ‚úÖ
- TTFT: **~2-3s** ‚úÖ
- Query time: **~8-10s**

### After Phase 3:
- Processing time: **~8-12s** ‚úÖ
- TTFT: **~1-2s** ‚úÖ
- Query time: **~5-8s**

## References

- [OpenAI Best Practices](https://platform.openai.com/docs/guides/production-best-practices)
- [RAG Optimization Guide](https://www.pinecone.io/learn/retrieval-augmented-generation/)
- [Vector Database Performance](https://www.pinecone.io/learn/vector-database/)
- [Async Python Best Practices](https://docs.python.org/3/library/asyncio-dev.html)

